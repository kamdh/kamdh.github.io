{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some libraries\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression # not allowed in homework!\n",
    "\n",
    "np.random.seed(0) # for reproducibility\n",
    "\n",
    "# generate data:\n",
    "n = 40\n",
    "d = 1000\n",
    "X = np.random.randn(n, d)\n",
    "beta = np.zeros(d)\n",
    "beta[0:4] = 3 * np.random.randn(4)\n",
    "y = X @ beta + 3.5 + 0.2 * np.random.randn(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity\n",
    "\n",
    "Either write a function that computes the $\\ell_0$ \"norm\" of a vector yourself, or lookup a built-in numpy function that does this for you.\n",
    "\n",
    "For the vector $\\vec \\beta$ we just created, compute $\\| \\vec\\beta \\|_0, \\| \\vec\\beta \\|_1, \\| \\vec\\beta \\|_2 $: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting an estimator\n",
    "\n",
    "Scikit-learn makes it quick and easy to fit a number of linear models to your data.\n",
    "For example, we can use ordinary least squares to estimate $\\vec \\beta_{\\rm OLS}$ by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "beta_OLS = model.fit(X, y).coef_\n",
    "print(beta_OLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the answer close to the truth?\n",
    "Why do you think OLS performs the way it does here?\n",
    "\n",
    "Now, you can fit a ridge or lasso model using the same syntax.\n",
    "Each of these has a regularizer either $\\lambda \\| \\vec\\beta \\|^2$ or $\\lambda \\| \\vec\\beta \\|_1$.\n",
    "In `sklearn`, the hyperparameter $\\lambda$ is called `alpha` to avoid confusion with python's `lambda` function.\n",
    "\n",
    "The follwing code uses ridge regression to estimate the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(alpha=1.)\n",
    "beta_ridge = model.fit(X, y).coef_\n",
    "print(beta_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso estimator\n",
    "\n",
    "Using the same syntax as for ridge, get the lasso estimate of the coefficient vector $\\vec\\beta$.\n",
    "Use `alpha = 1.`.\n",
    "Compare the sparsity to that of the OLS and ridge estimates, as well as the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "Go ahead an play with various values of `alpha`.\n",
    "It's best to tune hyperparameters on a logarithmic scale. The default was `alpha = 1.`, so try things like `1e-1, 1e-2, 1e1, 1e2` to cover multiple orders of magnitude.\n",
    "\n",
    "Look for the shrinkage (the estimated coefficients are \"shrunken\" values of the truth) and sparsity effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
